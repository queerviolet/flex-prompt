{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/queerviolet/flex-prompt/blob/main/doc/introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "nwBpcvzaqV3z",
        "outputId": "e47f63a0-a58d-442c-ac63-f3493de78724"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              "  .logo {\n",
              "    font-size: 600%;\n",
              "    text-align: center;\n",
              "    font-family: serif;\n",
              "    font-variant-ligatures: no-common-ligatures;\n",
              "  }\n",
              "</style>\n",
              "<div class=logo><i>flex</i>[prompt]</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title flex[prompt]\n",
        "%%html\n",
        "<style>\n",
        "  .logo {\n",
        "    font-size: 600%;\n",
        "    text-align: center;\n",
        "    font-family: serif;\n",
        "    font-variant-ligatures: no-common-ligatures;\n",
        "  }\n",
        "</style>\n",
        "<div class=logo><i>flex</i>[prompt]</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocKUc4sD9Kz2"
      },
      "source": [
        "Large language models have *maximum context window*—a maximum number of tokens they can receive and produce. You may have noticed this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwD6D3iu8Rs_"
      },
      "source": [
        "![Error message indiciating too many prompt tokens](./screenshot-max-content-length.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwDnYeFa8TIp"
      },
      "source": [
        "Flex prompt addresses this by fitting your prompt into the model's context window. You provide a flexible prompt template, flex prompt renders it into model input.\n",
        "\n",
        "Flex prompt does not handle model execution, but integrates well with execution frameworks like [LangChain](https://www.langchain.com/) and [Haystack](https://haystack.deepset.ai/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RMqhSRBhNyz",
        "outputId": "6b16be66-a71d-4b03-aa4a-957bcf1238f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting farm-haystack\n",
            "  Downloading farm_haystack-1.22.1-py3-none-any.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.0/856.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boilerpy3 (from farm-haystack)\n",
            "  Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n",
            "Collecting events (from farm-haystack)\n",
            "  Downloading Events-0.5-py3-none-any.whl (6.8 kB)\n",
            "Collecting httpx (from farm-haystack)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from farm-haystack) (4.19.2)\n",
            "Collecting lazy-imports==0.3.1 (from farm-haystack)\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from farm-haystack) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from farm-haystack) (3.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from farm-haystack) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from farm-haystack) (9.4.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from farm-haystack) (4.1.0)\n",
            "Collecting posthog (from farm-haystack)\n",
            "  Downloading posthog-3.1.0-py2.py3-none-any.whl (37 kB)\n",
            "Collecting prompthub-py==4.0.0 (from farm-haystack)\n",
            "  Downloading prompthub_py-4.0.0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: pydantic<2 in /usr/local/lib/python3.10/dist-packages (from farm-haystack) (1.10.13)\n",
            "Collecting quantulum3 (from farm-haystack)\n",
            "  Downloading quantulum3-0.9.0-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rank-bm25 (from farm-haystack)\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farm-haystack) (2.31.0)\n",
            "Collecting requests-cache<1.0.0 (from farm-haystack)\n",
            "  Downloading requests_cache-0.9.8-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=1.3.0 (from farm-haystack)\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sseclient-py (from farm-haystack)\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from farm-haystack) (8.2.3)\n",
            "Collecting tiktoken>=0.5.1 (from farm-haystack)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farm-haystack) (4.66.1)\n",
            "Collecting transformers==4.34.1 (from farm-haystack)\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from prompthub-py==4.0.0->farm-haystack) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1->farm-haystack) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1->farm-haystack) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1->farm-haystack) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1->farm-haystack) (23.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1->farm-haystack) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.1->farm-haystack)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1->farm-haystack) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2->farm-haystack) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack) (2023.11.17)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack) (1.4.4)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack) (23.1.0)\n",
            "Collecting cattrs>=22.2 (from requests-cache<1.0.0->farm-haystack)\n",
            "  Downloading cattrs-23.2.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting url-normalize>=1.4 (from requests-cache<1.0.0->farm-haystack)\n",
            "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->farm-haystack)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack) (1.3.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->farm-haystack)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack) (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->farm-haystack) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog->farm-haystack)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->farm-haystack)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from quantulum3->farm-haystack) (7.0.0)\n",
            "Collecting num2words (from quantulum3->farm-haystack)\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: exceptiongroup>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests-cache<1.0.0->farm-haystack) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1->farm-haystack) (2023.6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.1->farm-haystack)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from num2words->quantulum3->farm-haystack)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=01032eefc9669d44da4e7766199521ddbcf68171403c12d593c6b5b15009417b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: sseclient-py, monotonic, events, docopt, url-normalize, rank-bm25, num2words, lazy-imports, h11, cattrs, boilerpy3, backoff, tiktoken, scikit-learn, requests-cache, prompthub-py, posthog, huggingface-hub, httpcore, tokenizers, quantulum3, httpx, transformers, farm-haystack\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.19.4\n",
            "    Uninstalling huggingface-hub-0.19.4:\n",
            "      Successfully uninstalled huggingface-hub-0.19.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 boilerpy3-1.0.7 cattrs-23.2.3 docopt-0.6.2 events-0.5 farm-haystack-1.22.1 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 huggingface-hub-0.17.3 lazy-imports-0.3.1 monotonic-1.6 num2words-0.5.13 posthog-3.1.0 prompthub-py-4.0.0 quantulum3-0.9.0 rank-bm25-0.2.2 requests-cache-0.9.8 scikit-learn-1.3.2 sseclient-py-1.8.0 tiktoken-0.5.2 tokenizers-0.14.1 transformers-4.34.1 url-normalize-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install farm-haystack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3SS1dwbDhX6C"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import PromptModel, PromptNode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB2tby90iS-v",
        "outputId": "1ae45673-6b23-4a2e-f202-5b1ad4f3370b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt-3.5-turbo\n",
            "PromptModel({'_component_config': {'params': {'model_name_or_path': 'gpt-3.5-turbo', 'api_key': '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'}, 'type': 'PromptModel'}, 'outgoing_edges': 1, 'model_name_or_path': 'gpt-3.5-turbo', 'max_length': 100, 'api_key': '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', 'use_auth_token': None, 'use_gpu': None, 'devices': None, 'model_kwargs': {}, 'model_invocation_layer': <haystack.nodes.prompt.invocation_layer.chatgpt.ChatGPTInvocationLayer object at 0x7cdca86e8f40>})\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'results': [],\n",
              "  'invocation_context': {'query': 'hi, what are you doing?',\n",
              "   'results': [],\n",
              "   'prompts': []}},\n",
              " 'output_1')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "m = PromptModel(model_name_or_path='gpt-3.5-turbo', api_key=os.environ['OPENAI_API_KEY'])\n",
        "print(m.model_name_or_path)\n",
        "\n",
        "n = PromptNode(model_name_or_path=m)\n",
        "print(n.model_name_or_path)\n",
        "await n.arun('hi, what are you doing?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6hv4x_p-H3X"
      },
      "source": [
        "# Basics\n",
        "\n",
        "We'll install `flex-prompt` with the optional `openai` dependencies, since we're using OpenAI models for these examples. This will install `tiktoken`, the OpenAI tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrdFmLKcjCFy"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBPSgCuWlKjN",
        "outputId": "9d7d7a0f-cbd0-41d3-cd20-ce1b17067c1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/queerviolet/flex-prompt /tmp/pip-install-wmsa45k1/flex-prompt_c1806e191ebd4b2899b91de63ab788a2\n"
          ]
        }
      ],
      "source": [
        "#@title hot load flex-prompt\n",
        "#@markdown (note: change this to just !pip install before publishing)\n",
        "!pip uninstall -y flex-prompt >/dev/null 2>&1\n",
        "!pip install \"flex-prompt[openai] @ git+https://github.com/queerviolet/flex-prompt\" > /dev/null\n",
        "import sys\n",
        "delete = [s for s in sys.modules if s.startswith('flex_prompt')]\n",
        "for d in delete:\n",
        "  del sys.modules[d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0OYoangAo53"
      },
      "source": [
        "Let's get ourselves a long string to work with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "iroOB2oy56b5"
      },
      "outputs": [],
      "source": [
        "#@title `WAR_AND_PEACE` = *(text of War and Peace from Project Gutenberg)*\n",
        "WAR_AND_PEACE_URL = 'https://www.gutenberg.org/cache/epub/2600/pg2600.txt'\n",
        "from urllib.request import urlopen\n",
        "with urlopen(WAR_AND_PEACE_URL) as f: WAR_AND_PEACE = f.read().decode('utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llNgh6ddBJ-d"
      },
      "source": [
        "Flex prompt is largely agnostic to how you run your models. We'll use LangChain for these examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUgeP4fkbYgq"
      },
      "source": [
        "# Quick Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G5jX561jq3b"
      },
      "source": [
        "## Rendering directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08YaKEXOlTw6",
        "outputId": "501cb847-30dc-4163-eac4-c664a402f089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Leo Tolstoy/Tolstoi\n"
          ]
        }
      ],
      "source": [
        "from flex_prompt import render, Flex, Expect\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# LangChain integration is optional; you can also call\n",
        "# render with model=<model string> e.g. render(..., model='gpt-3.5-turbo')\n",
        "davinci = OpenAI()\n",
        "\n",
        "rendered = render(Flex([\n",
        "  'Given the text, answer the question.\\n',\n",
        "  'Text:', WAR_AND_PEACE,\n",
        "  'Question: Who wrote this?',\n",
        "  'Answer:', Expect()\n",
        "]), model=davinci)\n",
        "\n",
        "print(davinci(rendered.output, max_tokens=rendered.max_response_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H__skrVWW4OL"
      },
      "source": [
        "Here, we're using flex prompt's `Flex` component and rendering it directly. `Flex` divides the available space in the prompt evenly amongst its children, filling the space completely. Neat!\n",
        "\n",
        "But note that if we *entirely* fill the context window with our prompt, we'll have no more tokens left for the response! That's the role of `Expect`: it's a placeholder which participates in layout but doesn't render any tokens, leaving room for a response.\n",
        "\n",
        "We can get the rendered prompt string from `rendered.output` and the number of tokens available for the response from `rendered.max_response_tokens`, which we use to call the model and get an answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNiMhWctMgVC"
      },
      "source": [
        "## `Flexed` components\n",
        "\n",
        "You can inherit from `flex_prompt.Flexed` to define a prompt component whose `content()` is flexed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tSheF6aPOzGo"
      },
      "outputs": [],
      "source": [
        "from flex_prompt import Flexed, Expect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Ask(Flexed):\n",
        "  text: str\n",
        "  question: str\n",
        "  answer: str | Expect = Expect(str)\n",
        "  instruct: str = \"Given a text, answer the question.\"\n",
        "\n",
        "  flex_join = '\\n' # yielded items will be joined by newlines\n",
        "  def content(self, _ctx):\n",
        "    if self.instruct:\n",
        "      yield 'Given the text, answer the question.'\n",
        "      yield ''\n",
        "    yield 'Text:'\n",
        "    yield self.text\n",
        "    yield 'Question: ', self.question\n",
        "    yield 'Answer: ', self.answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndYSYn__R-p6"
      },
      "source": [
        "We can then pass an instance of `Ask` to `render`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqENjGZFPGAB",
        "outputId": "95175e3a-25f9-47fd-f130-2ae433151383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given the text, answer the question.\n",
            "\n",
            "Text:\n",
            ". He went up to Anna Pávlovna,\r\n",
            "kissed her hand, presenting to her his bald, scented, and shining head,\r\n",
            "and complacently seated himself on the sofa.\r\n",
            "\r\n",
            "“First of all, dear friend, tell me how you are. Set your friend’s\r\n",
            "mind at rest,” said he without altering his tone, beneath the\r\n",
            "politeness and affected sympathy of which indifference and even irony\r\n",
            "could be discerned.\r\n",
            "\r\n",
            "“Can one be well while suffering morally? Can one be calm in times\r\n",
            "like these if one has any feeling?�\n",
            "Question: What character names are in the text?\n",
            "Answer: \n",
            " Anna Pávlovna and the speaker.\n"
          ]
        }
      ],
      "source": [
        "from flex_prompt import render\n",
        "rendering = render(Ask(text=WAR_AND_PEACE[10000:], question=\"What character names are in the text?\"),\n",
        "                   model=davinci, token_limit=300)\n",
        "print(rendering.output)\n",
        "print(davinci(rendering.output, max_tokens=rendering.max_response_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJTQKq_USXgy"
      },
      "source": [
        "Note that we take an `answer` and default it to `Expect(str)`. Writing prompts like this lets us use the same component to render examples and the active prompt, simplifying format changes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndZqqge8S_k1",
        "outputId": "8f592e00-314b-4718-b448-5f0448c360de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given a text, answer the question.\n",
            "\n",
            "**EXAMPLE:**\n",
            "Text:\n",
            "The triangle is green\n",
            "Question: What color is the triangle?\n",
            "Answer: green\n",
            "**END EXAMPLE**\n",
            "**EXAMPLE:**\n",
            "Text:\n",
            "If you breathe deeply, you will fall asleep.\n",
            "Question: How do you fall asleep?\n",
            "Answer: breathe deeply\n",
            "**END EXAMPLE**\n",
            "**EXAMPLE:**\n",
            "Text:\n",
            "The 5-ht2a receptor mediates gastrointestinal activation\n",
            "Question: What does the 5-ht3 receptor do?\n",
            "Answer: not answered in the text\n",
            "**END EXAMPLE**\n",
            "Given the text, answer the question.\n",
            "\n",
            "Text:\n",
            "﻿The Project Gutenberg eBook of War and Peace\r\n",
            "    \r\n",
            "This ebook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "of the Project Gutenberg License included with this ebook or online\r\n",
            "at www.gutenberg.org. If you are not located in the United States,\r\n",
            "you will have to check the laws of the country where you are located\r\n",
            "before using this eBook.\r\n",
            "\r\n",
            "Title: War and Peace\r\n",
            "\r\n",
            "\r\n",
            "Author: graf Leo Tolstoy\r\n",
            "\r\n",
            "Translator: Aylmer Maude\r\n",
            "        Louise Maude\r\n",
            "\r\n",
            "Release date: April 1, 2001 [eBook #2600]\r\n",
            "                Most recently updated: June 14, 2022\r\n",
            "\r\n",
            "Language: English\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK WAR AND PEACE ***\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "WAR AND PEACE\r\n",
            "\r\n",
            "\r\n",
            "By Leo Tolstoy/Tolstoi\r\n",
            "\r\n",
            "\r\n",
            "    Contents\r\n",
            "\r\n",
            "    BOOK ONE: 1805\r\n",
            "\r\n",
            "    CHAPTER I\r\n",
            "\r\n",
            "    CHAPTER II\r\n",
            "\r\n",
            "    CHAPTER III\r\n",
            "\r\n",
            "    CHAPTER IV\r\n",
            "\r\n",
            "    CHAPTER V\r\n",
            "\r\n",
            "    CHAPTER VI\r\n",
            "\r\n",
            "    CHAPTER VII\r\n",
            "\r\n",
            "    CHAPTER VIII\r\n",
            "\r\n",
            "    CHAPTER IX\r\n",
            "\r\n",
            "    CHAPTER X\r\n",
            "\r\n",
            "    CHAPTER XI\r\n",
            "\r\n",
            "    CHAPTER XII\r\n",
            "\r\n",
            "    CHAPTER XIII\r\n",
            "\r\n",
            "    CHAPTER XIV\r\n",
            "\r\n",
            "    CHAPTER XV\r\n",
            "\r\n",
            "    CHAPTER XVI\r\n",
            "\r\n",
            "    CHAPTER XVII\r\n",
            "\r\n",
            "    CHAPTER\n",
            "Question: Who published this?\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "examples = [\n",
        "  ('The triangle is green', 'What color is the triangle?', 'green'),\n",
        "  ('If you breathe deeply, you will fall asleep.', 'How do you fall asleep?', 'breathe deeply'),\n",
        "  ('The 5-ht2a receptor mediates gastrointestinal activation',\n",
        "   'What does the 5-ht3 receptor do?',\n",
        "   'not answered in the text')\n",
        "]\n",
        "\n",
        "@dataclass\n",
        "class AskWithExamples(Flexed):\n",
        "  examples: list[tuple[str, str, str]]\n",
        "  ask: Ask\n",
        "\n",
        "  flex_join = '\\n'\n",
        "  def content(self, _ctx):\n",
        "    yield Ask.instruct\n",
        "    yield ''\n",
        "    for text, q, a in self.examples:\n",
        "      yield '**EXAMPLE:**'\n",
        "      yield Ask(text, q, a, instruct=None)\n",
        "      yield '**END EXAMPLE**'\n",
        "    yield self.ask\n",
        "\n",
        "\n",
        "rendering = render(\n",
        "    AskWithExamples(examples, Ask(WAR_AND_PEACE, 'Who published this?')),\n",
        "    model=davinci,\n",
        "    token_limit=1000)\n",
        "print(rendering.output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugEjLA6R_aWD"
      },
      "outputs": [],
      "source": [
        "#@title install langchain with OpenAI support\n",
        "#@markdown Note: You'll need an OPENAI_API_KEY defined in your\n",
        "#@markdown [colab secrets](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75).\n",
        "!pip install langchain openai > /dev/null\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A515OhxX_a7P"
      },
      "source": [
        "# `flex_prompt.render`\n",
        "Flex prompt exports a top-level `render` function which renders an input for a given model. This returns a `Rendering[str]`, whose `output` is the rendered prompt string.\n",
        "\n",
        "The input can be as simple as a string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd0w2ZZmjeNf",
        "outputId": "93e1a966-cc07-43ce-a396-6d75ec96f3c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Q: What are the colors of the rainbow?\\nA:', 4084)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from flex_prompt import render\n",
        "rendering = render(\"Q: What are the colors of the rainbow?\\nA:\", model='text-davinci-002')\n",
        "rendering.output, rendering.max_response_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlDjWKaLmLPF"
      },
      "source": [
        "The rendering has everything we need to call the model, here using LangChain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CzMh6GZ2mIpU",
        "outputId": "5b707a7e-89d0-45ab-fa2e-22b806620b07"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The colors of the rainbow are red, orange, yellow, green, blue, indigo, and violet.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "davinci = OpenAI(model='text-davinci-002')\n",
        "davinci(rendering.output, max_tokens=rendering.max_response_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onLRNKlIld_l"
      },
      "source": [
        "For convenience, the `render` function also accepts LangChain models directly:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QL4j8MDilbfj",
        "outputId": "ad01146b-94c4-4a26-f035-fe22be03ee68"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Red, orange, yellow, green, blue, and violet.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rendering = render(\"Q: What are the colors of the rainbow?\\nA:\", model=davinci)\n",
        "davinci(rendering.output, max_tokens=rendering.max_response_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6D0Nqn4p3T8"
      },
      "source": [
        "`render` will render most things you throw at it:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqHsmGGJp55T"
      },
      "source": [
        "## strings\n",
        "\n",
        "Rendered strings are cropped to fit. We can see this if we pass an artificially low `token_limit`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "y-0NYGo2qFRv",
        "outputId": "681b7077-a8ad-4ac2-ca53-564b0c664e3b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Q: What are the'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rendering = render(\"Q: What are the colors of the rainbow?\\nA:\", model=davinci, token_limit=5)\n",
        "rendering.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDGJpeNMy_hf"
      },
      "source": [
        "If the input overflowed, `overflow_token_count` will be non-zero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei9gCk9ly-iU",
        "outputId": "dc4aa866-a5c4-4494-a725-e7afae97b2e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rendering.overflow_token_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC6I3yYk7hIG"
      },
      "source": [
        "(You can't do much with this information right now, except know that the prompt overflowed)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWRjYQi2p1WS"
      },
      "source": [
        "## lists\n",
        "\n",
        "Rendering a `list` (or really, any non-`str` `Iterable`) concatenates all the items in that list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kAMoFrfpo1ea",
        "outputId": "84ce45b5-9311-4ef7-e960-05ec3228f294"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Answer the following questions:\\n1. Colors of the rainbow?\\n2. Days of the week?\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions = ['1. Colors of the rainbow?\\n', '2. Days of the week?\\n']\n",
        "rendering = render(['Answer the following questions:\\n', questions], model=davinci)\n",
        "rendering.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjbd1kCC8BZd"
      },
      "source": [
        "By default, lists are rendered in `block` mode. This means that partial items which would be cut off simply aren't rendered at all:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUuAq8sE8RZH",
        "outputId": "0f16c331-035d-44fe-8204-c85c1accf0be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "\n",
            "we were witness to dark and terrible portents, whose nameless\n",
            "features we could not grasp with our mortal minds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "one = \"\"\"\n",
        "and lo betide, the red sky opened upon us as though the crinkled\n",
        "hand of the heavens itself was reaching down.\n",
        "\"\"\"\n",
        "two = \"\"\"\n",
        "we were witness to dark and terrible portents, whose nameless\n",
        "features we could not grasp with our mortal minds\n",
        "\"\"\"\n",
        "three = \"\"\"\n",
        "it was only then, in the moment when cruel stars had long since\n",
        "wrung us dry, that the chinchillas arrived.\n",
        "\"\"\"\n",
        "\n",
        "rendering = render([one, two, three], model=davinci, token_limit=60)\n",
        "print(rendering.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mzExpx1cQ2s"
      },
      "source": [
        "To control this, you can use the `Cat` component explicitly (list rendering uses `Cat` implicitly)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYmpTBNQC8Ie"
      },
      "source": [
        "## callables (prompt components)\n",
        "\n",
        "If you `render` a callable, flex prompt will call it with a rendering context and expect it to return an iterable of rendered items. It's thus convenient define prompt components as callable dataclasses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sJXBnfQRFEKk",
        "outputId": "22eb0fcc-c833-4486-ff42-01e7b539a2b5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The text is from the Project Gutenberg eBook of War and Peace.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from flex_prompt import Flex, Render, Expect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Ask:\n",
        "  \"\"\"Given a text, answer a question.\"\"\"\n",
        "  text: str\n",
        "  question: str\n",
        "  def __call__(self, ctx: Render):\n",
        "    yield Flex([\n",
        "      'Given the text, answer the question\\n\\n',\n",
        "      'Text:\\n', self.text, '\\n',\n",
        "      'Question: ', self.question,\n",
        "      'Answer:', Expect()\n",
        "    ])\n",
        "\n",
        "rendered = render(Ask(text=WAR_AND_PEACE, question='Where is this text from?'), model=davinci)\n",
        "davinci(rendered.output, max_tokens=rendered.expected_token_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w74lDKciNjq6"
      },
      "source": [
        "In the example above, our prompt yields a single `Flex` with all our content. This is pretty common and regrettably ugly. Flex prompt provides a `Flexed` abstract base class to simplify the common case where you just want to throw a bunch of stuff in the prompt and have it show up. To use it, derive `Flexed` and implement `content()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "EFD6WRRoYsfJ",
        "outputId": "2d22e21b-fd47-4a87-e426-882d24710b96"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\r\\n\\r\\nThis ebook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever. It tells the story of War and Peace by Leo Tolstoy, and has\\r\\nbeen updated as recently as June 14, 2022.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from flex_prompt import Flex, Render, render, Flexed, Expect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Summarize(Flexed):\n",
        "  \"\"\"Summarize a text.\"\"\"\n",
        "  text: str\n",
        "\n",
        "  flex_join = '\\n' # If it's present in the class, Flexed will pass flex_join\n",
        "                  # to the inner Flex component\n",
        "  def content(self, _ctx: Render):\n",
        "    yield 'Summarize the text.'\n",
        "    yield 'Text:'\n",
        "    yield self.text\n",
        "    yield 'Summary:', Expect()\n",
        "\n",
        "rendered = render(Summarize(WAR_AND_PEACE), model=davinci)\n",
        "davinci(rendered.output, max_tokens=rendered.expected_token_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksNPCaxrZDr5"
      },
      "source": [
        "# Included components\n",
        "\n",
        "Flex prompt comes with a few components included.\n",
        "\n",
        "## `Flex`\n",
        "\n",
        "`Flex` divides the available space amongst its children:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciAHV3-Na4J7",
        "outputId": "ef964a14-b0b1-4447-9273-422c455d821f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AAAAAAAAAABBBBBBBBBBCCCCCCCCCC\n"
          ]
        }
      ],
      "source": [
        "from flex_prompt import render, Flex\n",
        "A = 'A' * 10000\n",
        "B = 'B' * 10000\n",
        "C = 'C' * 10000\n",
        "# the test-len-str model target is a test helper built into\n",
        "# flex prompt. its tokenizer just returns each character as a token.\n",
        "print(render(Flex([A, B, C]), model='test-len-str', token_limit=30).output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY0Zg4Dtdv_y"
      },
      "source": [
        "You can control how `Flex` divides the space with the `flex_weight` property, set on the child:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1GUB4Pxd-gb",
        "outputId": "26278c83-016f-4ca0-fa8d-7eea0dba056f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AAAAAAAAAABBBBBBBBBBCCCCCCCCCC\n",
            "AAAAAAABBBBBBBBBBBBBBBCCCCCCCC\n",
            "AAAAAABBBBBBBBBBBBBBBBBBCCCCCC\n",
            "AAAAABBBBBBBBBBBBBBBBBBBBCCCCC\n"
          ]
        }
      ],
      "source": [
        "for w in range(1, 5):\n",
        "  rendering = render(\n",
        "    Flex([A, Flex([B], flex_weight=w), C]),\n",
        "    model='test-len-str',\n",
        "    token_limit=30)\n",
        "  print(rendering.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ6cG1Lsf5Ip"
      },
      "source": [
        "You can specify a `join` argument to make `Flex` join its children while respecting the window size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2ZfkTyYf32s",
        "outputId": "df656ee2-cabc-4c5e-ed1d-56cfd3c84385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AAAAA\n",
            "--\n",
            "BBBBBBBBBBB\n",
            "--\n",
            "CCCCCC\n",
            "output length: 30\n"
          ]
        }
      ],
      "source": [
        "rendering = render(\n",
        "  Flex([A, Flex([B], flex_weight=2), C], join='\\n--\\n'),\n",
        "  model='test-len-str',\n",
        "  token_limit=30)\n",
        "print(rendering.output)\n",
        "print('output length:', len(rendering.output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QY6W_5k8aUv"
      },
      "source": [
        "## `Cat`\n",
        "\n",
        "Flex prompt's `Cat` component concatenates all the iterables you give it, and gives you more control over their rendering than if they were just in a list.\n",
        "\n",
        "With no arguments, it's equivalent to rendering a list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo4SCV-ogluH",
        "outputId": "5fc728a5-fecf-44c0-c55d-2caf995dc136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from flex_prompt import render, Cat\n",
        "rendering = render(Cat([one, two, three]), model=davinci, token_limit=40)\n",
        "print(rendering.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecAqYkVbgpxy"
      },
      "source": [
        "If you'd rather clip items which can't be completely rendered, you can specify `mode='clip'`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_NcCZ678hSw",
        "outputId": "5b304524-b106-46e8-d939-8c3abef36f76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "\n",
            "we were witness to dark and terrible portents,\n",
            "14 tokens clipped\n"
          ]
        }
      ],
      "source": [
        "from flex_prompt import render, Cat\n",
        "rendering = render(Cat([one, two, three], mode='clip'), model=davinci, token_limit=40)\n",
        "print(rendering.output)\n",
        "print(rendering.overflow_token_count, 'tokens clipped')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qlZXkAJ95re"
      },
      "source": [
        "`Cat` also lets you specify a `join`er, just as `Flex` does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dJOhCVD9_m8",
        "outputId": "30c99ab7-226c-4d9f-bf39-22b60261fd1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "---\n",
            "we were witness to dark and terrible portents, whose nameless\n",
            "features we could not grasp with our mortal minds\n",
            "---\n",
            "it was only then, in the moment when cruel stars had long\n"
          ]
        }
      ],
      "source": [
        "from flex_prompt import render, Cat\n",
        "rendering = render(Cat([one, two, three], mode='clip', join='---'), model=davinci, token_limit=70)\n",
        "print(rendering.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd80XGl_D7Od"
      },
      "source": [
        "# Targeting a particular model\n",
        "\n",
        "When you call `render(input, model=m)`, flex prompt searches for a render `Target` for `m`. If you want to do this search once rather than every time you render, you can call `flex_prompt.target` to get a model-specific renderer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pcW2Z7lEeAl",
        "outputId": "e8bcb5de-263b-4763-b990-850aeff04a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " They would most likely think very highly of the book as it is a Russian classic.\n"
          ]
        }
      ],
      "source": [
        "from flex_prompt import target, Flex, Expect\n",
        "render = target(davinci)\n",
        "rendering = render(Ask(text=WAR_AND_PEACE,\n",
        "                       question='What might a 19th century Russian aristocrat think of this book?'))\n",
        "print(davinci(rendering.output, max_tokens=rendering.expected_token_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfIHoOpghKyN"
      },
      "source": [
        "# Integrating new models\n",
        "\n",
        "When you ask flex prompt to render against a model, it looks up a `Target` for that model by calling a series of target finders. You can register a target finder using `flex_prompt.register_target_finder`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "n9XA2Js7hngf"
      },
      "outputs": [],
      "source": [
        "from flex_prompt import register_target_finder, Target\n",
        "from flex_prompt.rendering import Str\n",
        "from typing import Any\n",
        "\n",
        "class WordTokenizer:\n",
        "  def encode(self, string):\n",
        "    return list(self._encode(string))\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    return ''.join(tokens)\n",
        "\n",
        "  def _encode(self, string):\n",
        "    import re\n",
        "    start = 0\n",
        "    for m in re.finditer(r'(\\s|\\n)+', string):\n",
        "      space_start, space_end = m.span()\n",
        "      word = string[start:space_start]\n",
        "      if word: yield word\n",
        "      yield string[space_start:space_end]\n",
        "      start = space_end\n",
        "    yield string[start:]\n",
        "\n",
        "@register_target_finder\n",
        "def find_example_target(model: Any) -> Target | None:\n",
        "  if model == 'example-target':\n",
        "    return Target(10, WordTokenizer(), Str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX7JLBFhjPni",
        "outputId": "467b28d4-eab2-4cca-9aab-e88e7ec46763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "and lo betide, the red\n"
          ]
        }
      ],
      "source": [
        "from flex_prompt import render\n",
        "print(render(one, model='example-target').output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oONFedzRmazF"
      },
      "source": [
        "# Known Issues\n",
        "\n",
        "## Token accounting\n",
        "\n",
        "Flex prompt operates in token space: when you hand it strings to render, it  tokenizes them and then concatenates those token lists. It only finally generates a string when you read the rendering's `.output` property (or convert it to a string, which implicitly does the same thing). Flex prompt's layout engine assumes that `token_count(A + B) = token_count(A) + token_count(B)`.\n",
        "\n",
        "This makes layout a bit faster, since we avoid repeatedly calling the tokenizer as we concatenate substrings. Unfortunately, it's also incorrect.\n",
        "\n",
        "Specifically, if adjacent prompt fragments combine into a single token, flex prompt will report a `token_count` which is higher than the actual token count. You can see this by combining individual-character substrings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6sAnW5KlvPd",
        "outputId": "49c1a2b7-bb80-41c1-ac06-74c97a3546d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial rendering token count: 11\n",
            "actual token count: 2\n"
          ]
        }
      ],
      "source": [
        "from flex_prompt import target\n",
        "render = target('gpt-4')\n",
        "\n",
        "rendered = render([char for char in 'hello world']) # ['h', 'e', 'l', ...]\n",
        "print('initial rendering token count:', rendered.token_count)\n",
        "# rendered.output will be \"hello world\", so we're really just\n",
        "# rendering a single string here:\n",
        "print('actual token count:', render(rendered.output).token_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHJzNho5oMWd"
      },
      "source": [
        "Fortunately, this will always over-estimate the token count, so the fundamental guarantee that flex prompt fits prompts into the token window still holds. It is also *probably* a mistake to combine prompt sections in a way that could generate new words (that is, without whitespace), so this is unlikely to have a major impact in practice.\n",
        "\n",
        "If an absolutely accurate accounting of tokens is important for your use case, you should re-count tokens as in the example above."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOjEXCMp962SwyXSiuIY34m",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
