{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPYirjcNPuicytg9bG/WAAZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/queerviolet/flex-prompt/blob/main/doc/intro_to_flex_prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# flex[prompt]"
      ],
      "metadata": {
        "id": "DqD2ob7jvj36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (logo)\n",
        "%%html\n",
        "<style>\n",
        "  .logo {\n",
        "    font-size: 600%;\n",
        "    text-align: center;\n",
        "    font-family: serif;\n",
        "    font-variant-ligatures: no-common-ligatures;\n",
        "  }\n",
        "</style>\n",
        "<div class=logo><i>flex</i>[prompt]</div>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "cellView": "form",
        "id": "nwBpcvzaqV3z",
        "outputId": "c339448f-6e31-4e49-c692-025b6378105a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "  .logo {\n",
              "    font-size: 600%;\n",
              "    text-align: center;\n",
              "    font-family: serif;\n",
              "    font-variant-ligatures: no-common-ligatures;\n",
              "  }\n",
              "</style>\n",
              "<div class=logo><i>flex</i>[prompt]</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large language models have *maximum context window*—a maximum number of tokens they can receive and produce. You may have noticed this:"
      ],
      "metadata": {
        "id": "ocKUc4sD9Kz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Example error from exceeding a model's token limit](https://raw.githubusercontent.com/queerviolet/flex-prompt/main/doc/screenshot-max-content-length.png)\n"
      ],
      "metadata": {
        "id": "PwD6D3iu8Rs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flex prompt addresses this by fitting your prompt into the model's context window. You provide a flexible prompt template, flex prompt renders it into model input.\n",
        "\n",
        "Flex prompt does not handle model execution, but integrates well with execution frameworks like [LangChain](https://www.langchain.com/) and [Haystack](https://haystack.deepset.ai/)."
      ],
      "metadata": {
        "id": "fwDnYeFa8TIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quickstart"
      ],
      "metadata": {
        "id": "gUgeP4fkbYgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll install `flex-prompt` with the optional `openai` dependencies, since we're using OpenAI models for these examples. This will install `tiktoken`, the OpenAI tokenizer."
      ],
      "metadata": {
        "id": "U6hv4x_p-H3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBPSgCuWlKjN"
      },
      "outputs": [],
      "source": [
        "!pip install flex-prompt[openai]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also get ourselves a long string to work with:"
      ],
      "metadata": {
        "id": "o0OYoangAo53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title `WAR_AND_PEACE` = *(text of War and Peace from Project Gutenberg)*\n",
        "WAR_AND_PEACE_URL = 'https://www.gutenberg.org/cache/epub/2600/pg2600.txt'\n",
        "from urllib.request import urlopen\n",
        "with urlopen(WAR_AND_PEACE_URL) as f: WAR_AND_PEACE = f.read().decode('utf-8')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iroOB2oy56b5"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rendering directly"
      ],
      "metadata": {
        "id": "_G5jX561jq3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import render, Flex, Expect\n",
        "\n",
        "rendered = render(\n",
        "    Flex([\n",
        "      \"Given the text, answer the question.\",\n",
        "      \"--Text--\",\n",
        "      Flex([WAR_AND_PEACE], flex_weight=2),\n",
        "      \"--End Text--\",\n",
        "      \"Question: What's the title of this text?\",\n",
        "      \"Answer:\", Expect()\n",
        "    ], join='\\n'),\n",
        "    model='text-davinci-002',\n",
        "    # note: we're setting an artificially low token_limit for\n",
        "    # demonstration purposes. If you omit token_limit, flex_prompt\n",
        "    # will entirely fill the model's context window.\n",
        "    token_limit=300)\n",
        "\n",
        "print(rendered.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08YaKEXOlTw6",
        "outputId": "4f7c2feb-1ac2-4350-c3ae-65449fc26219"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given the text, answer the question.\n",
            "--Text--\n",
            "﻿The Project Gutenberg eBook of War and Peace\r\n",
            "    \r\n",
            "This ebook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "of the Project Gutenberg License included with this ebook or online\r\n",
            "at www.gutenberg.org. If you are not located in the United States,\r\n",
            "you will have to check the laws of the country where you are located\r\n",
            "before using this eBook.\r\n",
            "\r\n",
            "Title: War and Peace\r\n",
            "\r\n",
            "\r\n",
            "Author: graf Leo Tolstoy\r\n",
            "\r\n",
            "Translator: Aylmer Maude\r\n",
            "        Louise Maude\r\n",
            "\r\n",
            "Release date: April 1, 2001 [eBook\n",
            "--End Text--\n",
            "Question: What's the title of this text?\n",
            "Answer:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we're using flex prompt's `Flex` component and passing it directly to `render`. `Flex` divides the available space in the prompt evenly amongst its children, filling the space completely. Neat!\n",
        "\n",
        "But note that if we *entirely* fill the context window with our prompt, we'll have no more tokens left for the response! That's the role of `Expect`: it's a placeholder which participates in layout but doesn't render any tokens, leaving room for a response.\n",
        "\n",
        "We can get the rendered prompt string from `rendered.output`. The number of tokens available for the response is `rendered.max_response_tokens`:"
      ],
      "metadata": {
        "id": "H__skrVWW4OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rendered.max_response_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ137bSKgQv1",
        "outputId": "1eeec7e9-1d24-4495-912c-41ee45b1f43a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Flexed` components\n",
        "\n",
        "Rendering directly is fine for a quick example, but in practice you'll probably want prompts which can take parameters. You can inherit from `flex_prompt.Flexed` to define a prompt component whose `content()` is flexed:"
      ],
      "metadata": {
        "id": "eNiMhWctMgVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import Flexed, Expect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Ask(Flexed):\n",
        "  text: str\n",
        "  question: str\n",
        "  answer: str | Expect = Expect()\n",
        "  instruct: str = \"Given a text, answer the question.\"\n",
        "\n",
        "  flex_join = '\\n' # yielded items will be joined by newlines\n",
        "  def content(self, _ctx):\n",
        "    if self.instruct:\n",
        "      yield 'Given the text, answer the question.'\n",
        "      yield ''\n",
        "    yield '-- Begin Text --'\n",
        "    yield Flex([self.text], flex_weight=2)\n",
        "    yield '-- End Text --'\n",
        "    yield 'Question: ', self.question\n",
        "    yield 'Answer: ', self.answer"
      ],
      "metadata": {
        "id": "tSheF6aPOzGo"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then pass an instance of `Ask` to `render`:"
      ],
      "metadata": {
        "id": "ndYSYn__R-p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import render\n",
        "ask_tolstoy = Ask(text=WAR_AND_PEACE[10000:],\n",
        "                  question=\"What character names appear in the text?\")\n",
        "rendering = render(ask_tolstoy, model='gpt-4', token_limit=300)\n",
        "print(rendering.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqENjGZFPGAB",
        "outputId": "69d850c1-8203-41b2-fe4b-dec81da75bdc"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given the text, answer the question.\n",
            "\n",
            "-- Begin Text --\n",
            ". He went up to Anna Pávlovna,\r\n",
            "kissed her hand, presenting to her his bald, scented, and shining head,\r\n",
            "and complacently seated himself on the sofa.\r\n",
            "\r\n",
            "“First of all, dear friend, tell me how you are. Set your friend’s\r\n",
            "mind at rest,” said he without altering his tone, beneath the\r\n",
            "politeness and affected sympathy of which indifference and even irony\r\n",
            "could be discerned.\r\n",
            "\r\n",
            "“Can one be well while suffering morally? Can one be calm in times\r\n",
            "like these if one has any feeling?” said Anna Pávlovna. “You are\r\n",
            "staying the whole evening, I hope?”\r\n",
            "\r\n",
            "“And the fete at the English ambassador’s? Today is Wednesday. I\r\n",
            "must put in an appearance there,” said the prince. “My daughter is\r\n",
            "coming for me to take me there.”\n",
            "-- End Text --\n",
            "Question: What character names appear in the text?\n",
            "Answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we take an `answer` and default it to `Expect()` an answer from the LLM. Writing prompts like this lets us use the same component to render examples and the active prompt, simplifying format changes:"
      ],
      "metadata": {
        "id": "HJTQKq_USXgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AskWithExamples(Flexed):\n",
        "  examples: list[tuple[str, str, str]]\n",
        "  ask: Ask\n",
        "\n",
        "  flex_join = '\\n'\n",
        "  def content(self, _ctx):\n",
        "    yield Ask.instruct\n",
        "    yield ''\n",
        "    for text, q, a in self.examples:\n",
        "      yield '**EXAMPLE:**'\n",
        "      yield Ask(text, q, a, instruct=None)\n",
        "      yield '**END EXAMPLE**'\n",
        "    yield Flex([self.ask], flex_weight=2)\n",
        "\n",
        "examples = [\n",
        "  ('The triangle is green', 'What color is the triangle?', 'green'),\n",
        "  ('If you breathe deeply, you will fall asleep.', 'How do you fall asleep?', 'breathe deeply'),\n",
        "  ('The 5-ht2a receptor mediates gastrointestinal activation',\n",
        "   'What does the 5-ht3 receptor do?',\n",
        "   'not answered in the text')\n",
        "]\n",
        "\n",
        "ask_tolstoy_w_examples = AskWithExamples(examples, ask_tolstoy)\n",
        "rendering = render(\n",
        "    ask_tolstoy_w_examples,\n",
        "    model='gpt-4',\n",
        "    token_limit=500)\n",
        "print(rendering.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndZqqge8S_k1",
        "outputId": "ddf007aa-6ee5-405e-9f6f-95dfb1d88165"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given a text, answer the question.\n",
            "\n",
            "**EXAMPLE:**\n",
            "-- Begin Text --\n",
            "The triangle is green\n",
            "-- End Text --\n",
            "Question: What color is the triangle?\n",
            "Answer: green\n",
            "**END EXAMPLE**\n",
            "**EXAMPLE:**\n",
            "-- Begin Text --\n",
            "If you breathe deeply, you will fall asleep.\n",
            "-- End Text --\n",
            "Question: How do you fall asleep?\n",
            "Answer: breathe deeply\n",
            "**END EXAMPLE**\n",
            "**EXAMPLE:**\n",
            "-- Begin Text --\n",
            "The 5-ht2a receptor mediates gastrointestinal activation\n",
            "-- End Text --\n",
            "Question: What does the 5-ht3 receptor do?\n",
            "Answer: not answered in the text\n",
            "**END EXAMPLE**\n",
            "Given the text, answer the question.\n",
            "\n",
            "-- Begin Text --\n",
            ". He went up to Anna Pávlovna,\r\n",
            "kissed her hand, presenting to her his bald, scented, and shining head,\r\n",
            "and complacently seated himself on the sofa.\r\n",
            "\r\n",
            "“First of all, dear friend, tell me how you are. Set your friend’s\r\n",
            "mind at rest,” said he without altering his tone, beneath the\r\n",
            "politeness and affected sympathy of which indifference and even irony\r\n",
            "could be discerned.\r\n",
            "\r\n",
            "“Can one be well while suffering morally? Can one be calm in times\r\n",
            "like these if one has any feeling?” said Anna Pávlovna. “You are\r\n",
            "staying the whole evening, I hope?”\r\n",
            "\r\n",
            "“And the fete at the English ambassador’s? Today is Wednesday. I\r\n",
            "must put in an appearance there,” said the prince. “My daughter is\r\n",
            "coming for me to take me there.”\r\n",
            "\r\n",
            "“I thought today’s fete had been canceled. I confess all these\r\n",
            "festivities and fireworks are becoming wearisome.”\r\n",
            "\r\n",
            "“If they had known\n",
            "-- End Text --\n",
            "Question: What character names appear in the text?\n",
            "Answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution\n",
        "\n",
        "Flex prompt doesn't really care how you execute your prompt. But it does provide basic integration hooks: `render(model=)` accepts strings, LangChain models, and Haystack models. Note that not all models are supported out of the box. You can [register support for new models as](#scrollTo=AfIHoOpghKyN) needed."
      ],
      "metadata": {
        "id": "PYEUFL79jt9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (read our OpenAI key from the keychain)\n",
        "#@markdown We'll use OpenAI's models for these examples.\n",
        "#@markdown You'll need an OPENAI_API_KEY defined in your\n",
        "#@markdown [colab secrets](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75).\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "MBs6DJpGmnql"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using with LangChain"
      ],
      "metadata": {
        "id": "C4bE0_OhjcTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai"
      ],
      "metadata": {
        "id": "ugEjLA6R_aWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "rendering = render(ask_tolstoy_w_examples, model=llm)\n",
        "print(llm(rendering.output, max_tokens=rendering.max_response_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uesa5zCmkXPj",
        "outputId": "48109552-a97c-4391-9640-57082e6932dd"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Prince Vasíli, Anna Pávlovna Schérer, Baron Funke, Emperor Alexander, Novosíltsev, Hardenburg, Haugwitz, Wintzingerode, Vicomte de Mortemart, Abbé Morio, Anatole, Hippolyte, Princess Mary Bolkónskaya, Prince Bolkónski\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using with Haystack"
      ],
      "metadata": {
        "id": "Ic_6ytb2b2rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install farm-haystack"
      ],
      "metadata": {
        "id": "5RMqhSRBhNyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import PromptModel, PromptNode\n",
        "\n",
        "llm = PromptModel(model_name_or_path='text-davinci-002', api_key=os.environ['OPENAI_API_KEY'])\n",
        "rendering = render(ask_tolstoy_w_examples, model=llm)\n",
        "print(llm.invoke(rendering.output, max_tokens=rendering.max_response_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB2tby90iS-v",
        "outputId": "ccf645bb-d49e-48c5-b4af-205cb39168e3"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Anna Pávlovna, le Vicomte de Mortemart, the Abbé Morio, Baron Funke, Anatole, Hippolyte, Lavater, Princess Mary Bolkónskaya, and Prince Bolkónski.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `flex_prompt.render`\n",
        "Flex prompt exports a top-level `render` function which renders an input for a given model. This returns a `Rendering[str]`, whose `output` is the rendered prompt string.\n",
        "\n",
        "The input can be as simple as a string:"
      ],
      "metadata": {
        "id": "A515OhxX_a7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import render\n",
        "rendering = render(\"Q: What are the colors of the rainbow?\\nA:\", model='text-davinci-002')\n",
        "rendering.output, rendering.max_response_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd0w2ZZmjeNf",
        "outputId": "4d34dcc7-1b4e-4053-f84b-099decbf63d7"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Q: What are the colors of the rainbow?\\nA:', 4084)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rendering has everything we need to call the model, here using LangChain:"
      ],
      "metadata": {
        "id": "IlDjWKaLmLPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "davinci = OpenAI(model='text-davinci-002')\n",
        "davinci(rendering.output, max_tokens=rendering.max_response_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CzMh6GZ2mIpU",
        "outputId": "8ffba6b2-dd5f-44eb-832a-9e991c516e3b"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The colors of the rainbow are red, orange, yellow, green, blue, indigo, and violet.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenience, the `render` function also accepts LangChain and Haystack models directly:\n",
        "\n"
      ],
      "metadata": {
        "id": "onLRNKlIld_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rendering = render(\"Q: What are the colors of the rainbow?\\nA:\", model=davinci)\n",
        "davinci(rendering.output, max_tokens=rendering.max_response_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QL4j8MDilbfj",
        "outputId": "9c754e5c-a696-415f-ae42-d3ea6aeebf69"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The colors of the rainbow are red, orange, yellow, green, blue, indigo, and violet.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`render` will render most things you throw at it:"
      ],
      "metadata": {
        "id": "Y6D0Nqn4p3T8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## strings\n",
        "\n",
        "Rendered strings are cropped to fit. We can see this if we pass an artificially low `token_limit`:"
      ],
      "metadata": {
        "id": "pqHsmGGJp55T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rendering = render(\"Q: What are the colors of the rainbow?\\nA:\", model=davinci, token_limit=5)\n",
        "rendering.output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "y-0NYGo2qFRv",
        "outputId": "06d3da01-5b42-4f78-e2a0-28737a0f7dbe"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Q: What are the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the input overflowed, `overflow_token_count` will be non-zero:"
      ],
      "metadata": {
        "id": "kDGJpeNMy_hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rendering.overflow_token_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei9gCk9ly-iU",
        "outputId": "6add3989-32e2-4a27-853f-76fe523f7849"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(You can't do much with this information right now, except know that the prompt overflowed)."
      ],
      "metadata": {
        "id": "MC6I3yYk7hIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lists\n",
        "\n",
        "Rendering a `list` (or really, any non-`str` `Iterable`) concatenates all the items in that list:"
      ],
      "metadata": {
        "id": "hWRjYQi2p1WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = ['1. Colors of the rainbow?\\n', '2. Days of the week?\\n']\n",
        "rendering = render(['Answer the following questions:\\n', questions], model='gpt-4')\n",
        "rendering.output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kAMoFrfpo1ea",
        "outputId": "c9ccc0fb-77c4-450a-ceb9-0ec69c6dde5e"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer the following questions:\\n1. Colors of the rainbow?\\n2. Days of the week?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, lists are rendered in `block` mode. This means that partial items which would be cut off simply aren't rendered at all:"
      ],
      "metadata": {
        "id": "Bjbd1kCC8BZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one = \"\"\"\n",
        "and lo betide, the red sky opened upon us as though the crinkled\n",
        "hand of the heavens itself was reaching down.\n",
        "\"\"\"\n",
        "two = \"\"\"\n",
        "we were witness to dark and terrible portents, whose nameless\n",
        "features we could not grasp with our mortal minds\n",
        "\"\"\"\n",
        "three = \"\"\"\n",
        "it was only then, in the moment when cruel stars had long since\n",
        "wrung us dry, that the chinchillas arrived.\n",
        "\"\"\"\n",
        "\n",
        "rendering = render([one, two, three], model='gpt-4', token_limit=60)\n",
        "print(rendering.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUuAq8sE8RZH",
        "outputId": "b97b143e-e64e-4ec0-b24a-2c23cee1ea42"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "\n",
            "we were witness to dark and terrible portents, whose nameless\n",
            "features we could not grasp with our mortal minds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To control this, you can use the `Cat` component explicitly (list rendering uses `Cat` implicitly)."
      ],
      "metadata": {
        "id": "_mzExpx1cQ2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## callables (prompt components)\n",
        "\n",
        "If you `render` a callable, flex prompt will call it with a rendering context and expect it to return an iterable of rendered items. It's thus convenient define prompt components as callable dataclasses:"
      ],
      "metadata": {
        "id": "VYmpTBNQC8Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import Flex, Render, Expect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Ask:\n",
        "  \"\"\"Given a text, answer a question.\"\"\"\n",
        "  text: str\n",
        "  question: str\n",
        "  def __call__(self, ctx: Render):\n",
        "    yield Flex([\n",
        "      'Given the text, answer the question\\n\\n',\n",
        "      'Text:\\n', self.text, '\\n',\n",
        "      'Question: ', self.question, '\\n'\n",
        "      'Answer:', Expect()\n",
        "    ])\n",
        "\n",
        "rendered = render(Ask(text=[one, two, three],\n",
        "                      question='Where is this text from?'),\n",
        "                  model='text-davinci-002')\n",
        "print(rendered.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJXBnfQRFEKk",
        "outputId": "b91ffb68-48df-490c-9e8e-427879f94f3a"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given the text, answer the question\n",
            "\n",
            "Text:\n",
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "\n",
            "we were witness to dark and terrible portents, whose nameless\n",
            "features we could not grasp with our mortal minds\n",
            "\n",
            "it was only then, in the moment when cruel stars had long since\n",
            "wrung us dry, that the chinchillas arrived.\n",
            "\n",
            "Question: Where is this text from?\n",
            "Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example above, our prompt yields a single `Flex` with all our content. This is pretty common and regrettably ugly. Flex prompt provides a `Flexed` abstract base class to simplify the common case where you just want to throw a bunch of stuff in the prompt and have it show up. To use it, derive `Flexed` and implement `content()`:"
      ],
      "metadata": {
        "id": "w74lDKciNjq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import Flex, Render, render, Flexed, Expect\n",
        "from dataclasses import dataclass\n",
        "from typing import Any\n",
        "\n",
        "@dataclass\n",
        "class Summarize(Flexed):\n",
        "  \"\"\"Summarize a text.\"\"\"\n",
        "  text: Any\n",
        "\n",
        "  flex_join = '\\n' # If it's present in the class, Flexed will pass flex_join\n",
        "                  # to the inner Flex component\n",
        "  def content(self, _ctx: Render):\n",
        "    yield 'Summarize the text.'\n",
        "    yield 'Text:'\n",
        "    yield self.text\n",
        "    yield 'Summary:', Expect()\n",
        "\n",
        "rendered = render(Summarize([one, two, three]), model='text-davinci-002')\n",
        "print(rendered.output)\n",
        "print('expecting', rendered.max_response_tokens, 'tokens')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFD6WRRoYsfJ",
        "outputId": "8659e5b8-47e2-40de-9330-b4a5ea3736fb"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarize the text.\n",
            "Text:\n",
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "\n",
            "we were witness to dark and terrible portents, whose nameless\n",
            "features we could not grasp with our mortal minds\n",
            "\n",
            "it was only then, in the moment when cruel stars had long since\n",
            "wrung us dry, that the chinchillas arrived.\n",
            "\n",
            "Summary:\n",
            "expecting 4001 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Included components\n",
        "\n",
        "Flex prompt comes with a few components included."
      ],
      "metadata": {
        "id": "ksNPCaxrZDr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Flex`\n",
        "\n",
        "`Flex` divides the available space amongst its children:"
      ],
      "metadata": {
        "id": "xyYqw54hp9gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import render, Flex\n",
        "A = 'A' * 10000\n",
        "B = 'B' * 10000\n",
        "C = 'C' * 10000\n",
        "# the test-len-str model target is a test helper built into\n",
        "# flex prompt. its tokenizer just returns each character as a token.\n",
        "print(render(Flex([A, B, C]), model='test-len-str', token_limit=30).output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciAHV3-Na4J7",
        "outputId": "87baf5a7-b013-4d3a-e1dd-2d06ddf8030f"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AAAAAAAAAABBBBBBBBBBCCCCCCCCCC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can control how `Flex` divides the space with the `flex_weight` property, set on the child:"
      ],
      "metadata": {
        "id": "hY0Zg4Dtdv_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for w in range(1, 5):\n",
        "  rendering = render(\n",
        "    Flex([A, Flex([B], flex_weight=w), C]),\n",
        "    model='test-len-str',\n",
        "    token_limit=30)\n",
        "  print(rendering.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1GUB4Pxd-gb",
        "outputId": "60a4e7ce-a276-48bb-b249-842230cb9284"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AAAAAAAAAABBBBBBBBBBCCCCCCCCCC\n",
            "AAAAAAABBBBBBBBBBBBBBBCCCCCCCC\n",
            "AAAAAABBBBBBBBBBBBBBBBBBCCCCCC\n",
            "AAAAABBBBBBBBBBBBBBBBBBBBCCCCC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can specify a `join` argument to make `Flex` join its children while respecting the window size:"
      ],
      "metadata": {
        "id": "sJ6cG1Lsf5Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rendering = render(\n",
        "  Flex([A, Flex([B], flex_weight=2), C], join='\\n--\\n'),\n",
        "  model='test-len-str',\n",
        "  token_limit=30)\n",
        "print(rendering.output)\n",
        "print('output length:', len(rendering.output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2ZfkTyYf32s",
        "outputId": "207eb1dd-a40a-45e7-f54a-7a48cbbe3a92"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AAAAA\n",
            "--\n",
            "BBBBBBBBBBB\n",
            "--\n",
            "CCCCCC\n",
            "output length: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Cat`\n",
        "\n",
        "Flex prompt's `Cat` component concatenates all the iterables you give it, and gives you more control over their rendering than if they were just in a list.\n",
        "\n",
        "With no arguments, it's equivalent to rendering a list:"
      ],
      "metadata": {
        "id": "7QY6W_5k8aUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import render, Cat\n",
        "rendering = render(Cat([one, two, three]), model=davinci, token_limit=40)\n",
        "print(rendering.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo4SCV-ogluH",
        "outputId": "81f75b5e-6e15-46ad-a034-04d4c9407380"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you'd rather clip items which can't be completely rendered, you can specify `mode='clip'`:"
      ],
      "metadata": {
        "id": "ecAqYkVbgpxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import render, Cat\n",
        "rendering = render(Cat([one, two, three], mode='clip'), model=davinci, token_limit=40)\n",
        "print(rendering.output)\n",
        "print(rendering.overflow_token_count, 'tokens clipped')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_NcCZ678hSw",
        "outputId": "eab11a9f-3a9b-414b-edc2-d1cc93a0e149"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "\n",
            "we were witness to dark and terrible portents,\n",
            "14 tokens clipped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Cat` also lets you specify a `join`er, just as `Flex` does:"
      ],
      "metadata": {
        "id": "7qlZXkAJ95re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import render, Cat\n",
        "rendering = render(Cat([one, two, three], mode='clip', join='---'), model=davinci, token_limit=70)\n",
        "print(rendering.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dJOhCVD9_m8",
        "outputId": "2a52a426-5e4c-47ef-8617-248c32ffdba7"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "---\n",
            "we were witness to dark and terrible portents, whose nameless\n",
            "features we could not grasp with our mortal minds\n",
            "---\n",
            "it was only then, in the moment when cruel stars had long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Expect`\n",
        "\n",
        "`Expect` is a layout placeholder. It participates in layout but doesn't produce any tokens, leaving space for model output:"
      ],
      "metadata": {
        "id": "gCtlMSjqxvXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import render, Flex, Expect\n",
        "\n",
        "# without Expect\n",
        "rendering = render(Flex([\n",
        "  'What is the sentiment of the following text?',\n",
        "  'Text:', Cat([one, two, three], mode='clip'),\n",
        "  'Sentiment:'\n",
        "], join='\\n'), model='gpt-4', token_limit=80)\n",
        "print('without Expect:')\n",
        "print(rendering.output)\n",
        "# note that this may not be exactly zero due to Flex rounding\n",
        "# and [token accounting](#scrollTo=oONFedzRmazF)\n",
        "print('available response tokens:', rendering.max_response_tokens)\n",
        "\n",
        "# with Expect\n",
        "rendering = render(Flex([\n",
        "  'What is the sentiment of the following text?',\n",
        "  'Text:', Cat([one, two, three], mode='clip'),\n",
        "  'Sentiment:', Expect()\n",
        "], join='\\n'), model='gpt-4', token_limit=80)\n",
        "print('\\n\\nwith Expect:')\n",
        "print(rendering.output)\n",
        "print('available response tokens:', rendering.max_response_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCIGF2i5zj5D",
        "outputId": "c96c94e7-cfb5-404a-942b-2ac4a2857a55"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "without Expect:\n",
            "What is the sentiment of the following text?\n",
            "Text:\n",
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "\n",
            "we were witness to dark and terrible portents, whose nameless\n",
            "features we could not grasp with our mortal minds\n",
            "\n",
            "it was only then, in the moment when\n",
            "Sentiment:\n",
            "available response tokens: 5\n",
            "\n",
            "\n",
            "with Expect:\n",
            "What is the sentiment of the following text?\n",
            "Text:\n",
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "\n",
            "we were\n",
            "Sentiment:\n",
            "\n",
            "available response tokens: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like all other built in components, `Expect` takes `flex_weight`:"
      ],
      "metadata": {
        "id": "I0iIEJhd2CYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for w in range(1, 5):\n",
        "  rendering = render(\n",
        "    Flex([A, B, C, Expect(flex_weight=w)]),\n",
        "    model='test-len-str',\n",
        "    token_limit=30)\n",
        "  print(rendering.output, f'{rendering.max_response_tokens=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s3A8D5n2Joq",
        "outputId": "2f64e9fd-121b-4523-a7b5-201faa039977"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AAAAAAABBBBBBBCCCCCCCC rendering.max_response_tokens=8\n",
            "AAAAAABBBBBBCCCCCC rendering.max_response_tokens=12\n",
            "AAAAABBBBBCCCCC rendering.max_response_tokens=15\n",
            "AAAABBBBCCCC rendering.max_response_tokens=18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting a model-specific render function\n",
        "\n",
        "When you call `render(input, model=m)`, flex prompt searches for a render `Target` for `m`. Finders may need to look up model parameters, which could be an expensive operation. If you want to do this search once rather than every time you render, you can call `flex_prompt.target` to get a model-specific renderer:"
      ],
      "metadata": {
        "id": "fd80XGl_D7Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import target, Flex, Expect\n",
        "render = target(davinci)\n",
        "rendering = render(Ask(text=WAR_AND_PEACE,\n",
        "                       question='What might a 19th century Russian aristocrat think of this book?'))\n",
        "print(davinci(rendering.output, max_tokens=rendering.max_response_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pcW2Z7lEeAl",
        "outputId": "44502c58-3f01-4b1f-88a1-e7598c71a667"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The aristocrat might think the book is well-written and informative, but they may not agree with Tolstoy's views on war and peace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is equivalent to calling `render(model=)` (and is how `render` is implemented internally)."
      ],
      "metadata": {
        "id": "IwwqxKyl26AZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating new models\n",
        "\n",
        "When you ask flex prompt to render against a model, it looks up a `Target` for that model by calling a series of target finders. You can register a target finder using `flex_prompt.register_target_finder`:"
      ],
      "metadata": {
        "id": "AfIHoOpghKyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import register_target_finder, Target\n",
        "from flex_prompt.rendering import Str\n",
        "from typing import Any\n",
        "\n",
        "class WordTokenizer:\n",
        "  def encode(self, string):\n",
        "    return list(self._encode(string))\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    return ''.join(tokens)\n",
        "\n",
        "  def _encode(self, string):\n",
        "    import re\n",
        "    start = 0\n",
        "    for m in re.finditer(r'(\\s|\\n)+', string):\n",
        "      space_start, space_end = m.span()\n",
        "      word = string[start:space_start]\n",
        "      if word: yield word\n",
        "      yield string[space_start:space_end]\n",
        "      start = space_end\n",
        "    yield string[start:]\n",
        "\n",
        "@register_target_finder\n",
        "def find_example_target(model: Any) -> Target | None:\n",
        "  if model == 'example-target':\n",
        "    return Target(10, WordTokenizer(), Str)\n",
        "  elif model == 'example-target-big':\n",
        "    return Target(100, WordTokenizer(), Str)"
      ],
      "metadata": {
        "id": "n9XA2Js7hngf"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import render\n",
        "print(render(one, model='example-target').output)\n",
        "print(render(one, model='example-target-big').output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX7JLBFhjPni",
        "outputId": "0620d15a-545f-4219-836b-ec25a3f948ef"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "and lo betide, the red\n",
            "\n",
            "and lo betide, the red sky opened upon us as though the crinkled\n",
            "hand of the heavens itself was reaching down.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Known Issues\n",
        "\n",
        "## Token accounting\n",
        "\n",
        "Flex prompt operates in token space: when you hand it strings to render, it  tokenizes them and then concatenates those token lists. It only finally generates a string when you read the rendering's `.output` property (or convert it to a string, which implicitly does the same thing). Flex prompt's layout engine assumes that `token_count(A + B) = token_count(A) + token_count(B)`.\n",
        "\n",
        "This makes layout a bit faster, since we avoid repeatedly calling the tokenizer as we concatenate substrings. Unfortunately, it's also incorrect.\n",
        "\n",
        "Specifically, if adjacent prompt fragments combine into a single token, flex prompt will report a `token_count` which is higher than the actual token count. You can see this by combining individual-character substrings:"
      ],
      "metadata": {
        "id": "oONFedzRmazF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flex_prompt import target\n",
        "render = target('gpt-4')\n",
        "\n",
        "rendered = render([char for char in 'hello world']) # ['h', 'e', 'l', ...]\n",
        "print('initial rendering token count:', rendered.token_count)\n",
        "# rendered.output will be \"hello world\", so we're really just\n",
        "# rendering a single string here:\n",
        "print('actual token count:', render(rendered.output).token_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6sAnW5KlvPd",
        "outputId": "b65b91bf-0055-4b73-9c4d-83b640eb7ec9"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial rendering token count: 11\n",
            "actual token count: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fortunately, this will always over-estimate the token count, so the fundamental guarantee that flex prompt fits prompts into the token window still holds. It is also *probably* a mistake to combine prompt sections in a way that could generate new words (that is, without whitespace), so this is unlikely to have a major impact in practice.\n",
        "\n",
        "If an absolutely accurate accounting of tokens is important for your use case, you should re-count tokens as in the example above."
      ],
      "metadata": {
        "id": "vHJzNho5oMWd"
      }
    }
  ]
}